{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgGc4qaTjPrU"
      },
      "source": [
        "Welcome to assignment 1.                                                       \n",
        "\n",
        "We are using pathology images for our first assignment please download data from this link https://drive.google.com/drive/folders/10dUOzcPR-PQwfFYcHk5gsLjIjSorQ32Q?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s4K2S-dhTnF"
      },
      "source": [
        "\n",
        "\n",
        "# Task 1: Feature Generation (15%)\n",
        "# Use and run the following code (a deep network) to generate features from a set of training images. For this assignment, you do not need to know how the deep network is working here to extract features.\n",
        "# This code extracts the features of image T4.tif (in the T folder of dataset). Modify the code so that it iterates over all images of the dataset and extracts their features.\n",
        "# Allocate 10% of the data for validation.\n",
        "\n",
        "# Insert your code here for Task 1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "94c_H8Yv7IDs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "780\n",
            "Test Set Length: 78\n",
            "Train Set Length: 702\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import densenet121\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Load pre-trained DenseNet model\n",
        "model = densenet121(weights='DenseNet121_Weights.IMAGENET1K_V1')\n",
        "\n",
        "# Remove the classification layer (last fully connected layer)\n",
        "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
        "\n",
        "# Add a global average pooling layer\n",
        "model.add_module('global_avg_pool', torch.nn.AdaptiveAvgPool2d(1))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Define the image preprocessing pipeline\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load dataset folder\n",
        "imageNames = []\n",
        "extractedFeatures = []\n",
        "\n",
        "#shouldn't need to change the root directory if you pulled correctly\n",
        "for root, _, files in os.walk('.\\\\train-20240206T024149Z-001\\\\train'):\n",
        "    for file in files:\n",
        "            imageNames.append(os.path.join(root, file))\n",
        "            # Load an image\n",
        "            image_path = os.path.join(root, file)\n",
        "\n",
        "            # get_image_files(image_path)\n",
        "\n",
        "            image = Image.open(image_path)\n",
        "\n",
        "            # Preprocess the image\n",
        "            input_tensor = preprocess(image)\n",
        "            input_batch = input_tensor.unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "            # Wrap the input tensor in a Variable\n",
        "            input_var = Variable(input_batch)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            features = model(input_var)\n",
        "\n",
        "            # Extract the feature tensor\n",
        "            feature_vector = features.squeeze().detach().numpy()\n",
        "\n",
        "            # Now 'feature_vector' contains the feature from the last fully connected layer of DenseNet\n",
        "            #print(\"Feature vector shape:\", feature_vector.shape)\n",
        "            # for feature in feature_vector:\n",
        "            #     print(feature)\n",
        "            extractedFeatures.append(feature_vector)\n",
        "\n",
        "#check to see if all images were parsed through\n",
        "print(len(extractedFeatures))\n",
        "\n",
        "# Needed to convert to np array. was having issues with just normal list\n",
        "extractedFeatures = np.array(extractedFeatures)\n",
        "\n",
        "# Splitting the data\n",
        "tenPercentLength = int(len(extractedFeatures) * 0.1)\n",
        "randomIndices = np.random.choice(len(extractedFeatures), size=tenPercentLength, replace=False)\n",
        "\n",
        "# Extract the elements corresponding to the random indices for test set\n",
        "validationFeatures = extractedFeatures[randomIndices]\n",
        "validationImageNames = [imageNames[i] for i in randomIndices]\n",
        "\n",
        "# Extract the remaining elements for training set\n",
        "trainFeatures = np.delete(extractedFeatures, randomIndices, axis=0)\n",
        "trainImageNames = [imageNames[i] for i in range(len(imageNames)) if i not in randomIndices]\n",
        "\n",
        "# I (Jared) needed this for KMeans (if anyone else needs this I (Jared) can explain how it works)\n",
        "# Get the letter labels for validation and training sets\n",
        "validationLabels = [label.split('\\\\')[-2][-1] for label in validationImageNames]\n",
        "trainLabels = [label.split('\\\\')[-2][-1] for label in trainImageNames]\n",
        "\n",
        "# Create a mapping between letter labels and numeric labels\n",
        "unique_labels = np.unique(trainLabels)\n",
        "label_mapping = {label: i for i, label in enumerate(unique_labels)}\n",
        "\n",
        "# Convert letter labels to numeric labels\n",
        "validationLabelsNum = [label_mapping[label] for label in validationLabels]\n",
        "trainLabelsNum = [label_mapping[label] for label in trainLabels]\n",
        "\n",
        "print('Test Set Length:', len(validationFeatures))\n",
        "print('Train Set Length:', len(trainFeatures))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DguMbSShmHT"
      },
      "source": [
        "# Task 2: High Bias Classification Method (5%)\n",
        "# Choose a classification method and let is have a high bias.\n",
        "# Train it on the generated features and discuss why it is underfitting.\n",
        "\n",
        "# Insert your code here for Task 2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xG7aIh1lhpW3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR8MxxoGhpxF"
      },
      "source": [
        "# Task 3: High Variance Classification Method (5%)\n",
        "# Use the chosen classification method and let it have a high variance.\n",
        "# Train it on the generated features and discuss why it is overfitting.\n",
        "\n",
        "# Insert your code here for Task 3\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrsSDN_7huYB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzxSVPWXht-m"
      },
      "source": [
        "# Task 4: Balanced Classification Method (15%)\n",
        "# Use the chosen classification method and let it balance the bias and variance.\n",
        "# Train it on the generated features, possibly adjusting parameters.\n",
        "# Discuss insights into achieving balance.\n",
        "\n",
        "# Insert your code here for Task 4\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjgmSxk7h7vZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKRG3PfFh8Ot"
      },
      "source": [
        "# Task 5: K-Means Clustering (20%)\n",
        "# Apply K-Means clustering on the generated features.\n",
        "# Test with available labels and report accuracy.\n",
        "# Experiment with automated K and compare with manually set 20 clusters.\n",
        "\n",
        "# Insert your code here for Task 5\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "VLuOkJyAh-mN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy K (20) =  0.7948717948717948\n",
            "Automated K Value =  17\n",
            "Accuracy K (Auto) =  0.782051282051282\n"
          ]
        }
      ],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "##### HELPER FUNCTIONS #####\n",
        "\n",
        "# helper function to map the labels from the KMeans data to the training data\n",
        "def map_labels(train_labels, cluster_labels, predict_labels):\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    conf_matrix = confusion_matrix(train_labels, cluster_labels)\n",
        "\n",
        "    # Apply the Hungarian algorithm to find the best matching\n",
        "    row_ind, col_ind = linear_sum_assignment(-conf_matrix)\n",
        "\n",
        "    # Map KMeans labels to train labels based on the best matching\n",
        "    label_mapping = {k: v for k, v in zip(col_ind, row_ind)}\n",
        "\n",
        "    # Map prediction (KMean) labels to original labels\n",
        "    prediction_mapped = [label_mapping[label] for label in predict_labels]\n",
        "\n",
        "    return prediction_mapped\n",
        "\n",
        "# helper function to auto select the value of K (clusters) using the elbow method\n",
        "def autoK(features, max_clusters):\n",
        "    wcss = []\n",
        "    \n",
        "    # iterate through K = 2, 3, ..., max_clusters + 1 and keep track of WCSS value for each\n",
        "    for i in range(2, max_clusters + 1):\n",
        "        kmeans = KMeans(n_clusters = i, init='k-means++', random_state = 420)\n",
        "        kmeans.fit(features)\n",
        "        wcss.append(kmeans.inertia_)\n",
        "\n",
        "    # Calculate the first derivative of the WCSS and use it to find the elbow point\n",
        "    differences = np.diff(wcss)\n",
        "    elbow_point = np.argmax(differences) + 1\n",
        "        \n",
        "    return elbow_point\n",
        "\n",
        "##### MAIN CODE #####\n",
        "\n",
        "# Apply KMeans clustering to data using K clusters = 20\n",
        "kmeans_man = KMeans(n_clusters = 20, random_state = 420)\n",
        "kmeans_man.fit(trainFeatures)\n",
        "\n",
        "# predict test data and map the predicted clusters numbers to the corresponding trained cluster numbers\n",
        "prediction_man = kmeans_man.predict(validationFeatures)\n",
        "prediction_man_mapped = map_labels(trainLabelsNum, kmeans_man.labels_, prediction_man)\n",
        "\n",
        "# check accuracy\n",
        "accuracy = accuracy_score(validationLabelsNum, prediction_man_mapped)\n",
        "print(\"Accuracy K (20) = \", accuracy)\n",
        "\n",
        "# set K (auto)\n",
        "K = autoK(trainFeatures, max_clusters = 40)\n",
        "print(\"Automated K Value = \", K)\n",
        "\n",
        "# Apply KMeans clustering to data using K clusters = auto\n",
        "kmeans_auto = KMeans(n_clusters = K, random_state = 420)\n",
        "kmeans_auto.fit(trainFeatures)\n",
        "\n",
        "# predict test data and map the predicted clusters numbers to the corresponding trained cluster numbers\n",
        "prediction_auto = kmeans_auto.predict(validationFeatures)\n",
        "prediction_auto_mapped = map_labels(trainLabelsNum, kmeans_auto.labels_, prediction_auto)\n",
        "\n",
        "# check accuracy\n",
        "accuracy = accuracy_score(validationLabelsNum, prediction_auto_mapped)\n",
        "print(\"Accuracy K (Auto) = \", accuracy)\n",
        "\n",
        "# plot results\n",
        "#fig = plt.figure(2, figsize = (11,4))\n",
        "\n",
        "# K = 20\n",
        "#ax = fig.add_subplot(121, projection='3d')\n",
        "#ax.scatter(trainFeatures[:,0],trainFeatures[:,1], trainFeatures[:,2], c=labels_man, cmap='Set2')\n",
        "#plt.title('KMeans using sklearn, K = 20')\n",
        "\n",
        "# auto K\n",
        "#ax = fig.add_subplot(122,projection='3d')\n",
        "#ax.scatter(trainFeatures[:,0],trainFeatures[:,1], trainFeatures[:,2], c=labels_auto, cmap='Set2')\n",
        "#plt.title('KMeans using sklearn, K = auto')\n",
        "#plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43sPfI7Jh-9p"
      },
      "source": [
        "# Task 6: Additional Clustering Algorithm (10%)\n",
        "# Choose another clustering algorithm and apply it on the features.\n",
        "# Test accuracy with available labels.\n",
        "\n",
        "# Insert your code here for Task 6\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nn9f41LWiCDr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fPfoBsaiCXu"
      },
      "source": [
        "# Task 7: PCA for Classification Improvement (20%)\n",
        "# Apply PCA on the features and then feed them to the best classification method in the above tasks.\n",
        "# Assess if PCA improves outcomes and discuss the results.\n",
        "\n",
        "# Insert your code here for Task 7\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoOFXhdmiHeD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQqNra7eiHx-"
      },
      "source": [
        "# Task 8: Visualization and Analysis (10%)\n",
        "# Plot the features in a lower dimension using dimentinality reduction techniques.\n",
        "# Analyze the visual representation, identifying patterns or insights.\n",
        "\n",
        "# Insert your code here for Task 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1npTL_NkjNdL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
